\# K8s ML Predictive Autoscaling



Исследовательский и инженерный проект по \*\*ML-основанному предиктивному автомасштабированию\*\* микросервисов в Kubernetes.



Цель — перейти от классического \*\*реактивного масштабирования (HPA по CPU/памяти)\*\* к \*\*проактивному планированию ресурсов на основе прогноза нагрузки\*\*, чтобы:



\* удерживать латентность и SLO под контролем;

\* снижать инфраструктурные затраты;

\* получить воспроизводимую экспериментальную установку для научной работы (НИР, курсовая, ВКР, публикация).



> Кратко: здесь исследуется, насколько модели Prophet / LSTM / гибрид Prophet+LSTM могут улучшить работу Kubernetes-автомасштабирования по сравнению с обычным HPA.



---



\## 1. Мотивация и постановка задачи



Современные системы автомасштабирования (например, Kubernetes Horizontal Pod Autoscaler) в основном используют \*\*реактивный подход\*\*: решение о масштабировании принимается по текущим метрикам CPU/памяти, часто с задержкой 30–60 секунд. Это приводит к:



\* временной деградации производительности при всплесках нагрузки;

\* росту латентности и нарушению SLA/SLO;

\* эффекту «осцилляций» (частое масштабирование вверх/вниз);

\* неоптимальному использованию ресурсов и лишним затратам.



\*\*Предиктивное масштабирование\*\* использует методы машинного обучения для прогноза нагрузки на горизонте нескольких минут и проактивного изменения количества реплик и лимитов ресурсов до наступления пиков.



В рамках проекта рассматриваются и сравниваются подходы:



\* Реактивный HPA (baseline).

\* Прогнозирование временных рядов: LSTM / GRU.

\* Prophet (модель с учётом тренда и сезонности).

\* Гибрид Prophet + LSTM.

\* (Опционально) RL-подходы к автомасштабированию.



---



\## 2. Цели и задачи проекта



\### Цель



Разработать и исследовать систему предиктивного автомасштабирования микросервисов в Kubernetes на основе ML-прогноза нагрузки и сравнить её с классическим HPA по метрикам качества, производительности и стоимости.



\### Основные задачи



1\. Собрать и подготовить временные ряды нагрузки из Prometheus (CPU, память, RPS, latency, ошибки и т.д.).

2\. Обучить и сравнить модели Prophet, LSTM/GRU и гибрид Prophet+LSTM по точности прогноза.

3\. Реализовать сервис онлайн-прогноза нагрузки (FastAPI + ONNX Runtime).

4\. Разработать модуль планирования ресурсов (Resource Planner), который переводит прогноз в:



&nbsp;  \* количество реплик;

&nbsp;  \* CPU/memory requests \& limits;

&nbsp;  \* политики ramp-up / ramp-down.

5\. Интегрировать ML-прогнозы с Kubernetes через:



&nbsp;  \* HPA external/custom metrics;

&nbsp;  \* KEDA (event-driven autoscaling).

6\. Построить Grafana-дашборд для визуализации:



&nbsp;  \* реальной нагрузки vs. прогноз;

&nbsp;  \* решений по масштабированию;

&nbsp;  \* экономии ресурсов и SLO-нарушений.

7\. Провести серию экспериментов с нагрузочным тестированием и формализовать результаты для научной публикации.



---



\## 3. Общая архитектура



Система следует паттерну \*\*MAPE loop\*\*: \*\*Monitor → Analyze → Plan → Execute\*\*.



1\. \*\*Monitor\*\*



&nbsp;  \* Микросервисы экспонируют `/metrics`.

&nbsp;  \* Prometheus собирает:



&nbsp;    \* CPU, память;

&nbsp;    \* network I/O;

&nbsp;    \* request rate;

&nbsp;    \* latency (p50, p95, p99);

&nbsp;    \* error rate, custom-метрики приложения.

&nbsp;  \* Данные сохраняются как временные ряды и экспортируются для обучения/инференса.



2\. \*\*Analyze (ML-слой)\*\*



&nbsp;  \* Препроцессинг данных:



&nbsp;    \* агрегация по фиксированному шагу (10–60 секунд);

&nbsp;    \* фильтрация аномалий (DDoS, synthetic load, ошибки мониторинга);

&nbsp;    \* sliding window генерация;

&nbsp;    \* опционально — декомпозиция временных рядов (например, CEEMDAN).

&nbsp;  \* Модели прогнозирования:



&nbsp;    \* LSTM / GRU для нелинейных паттернов;

&nbsp;    \* Prophet для трендов и сезонности;

&nbsp;    \* гибрид Prophet+LSTM (Prophet моделирует тренд/сезонность, LSTM — остатки).

&nbsp;  \* Выход: прогноз нагрузки на горизонте 5–30 минут + доверительные интервалы.



3\. \*\*Plan (Resource Planner)\*\*



&nbsp;  \* Конвертирует прогноз в план ресурсов:



&nbsp;    \* целевое количество реплик;

&nbsp;    \* лимиты CPU/памяти;

&nbsp;    \* стратегия ramp-up / ramp-down;

&nbsp;    \* небольшой over-provisioning (например, 10–15%) для надёжности.

&nbsp;  \* Учитывает бизнес-ограничения:



&nbsp;    \* минимальное/максимальное число реплик;

&nbsp;    \* SLO по латентности;

&nbsp;    \* бюджет ресурсов.



4\. \*\*Execute (интеграция с Kubernetes)\*\*



&nbsp;  \* ML-прогнозы экспортируются как \*\*external/custom metrics\*\* для HPA либо как источник для KEDA.

&nbsp;  \* Через Kubernetes API обновляются:



&nbsp;    \* объекты `HorizontalPodAutoscaler`;

&nbsp;    \* `ScaledObject` (для KEDA) или кастомные CRD.

&nbsp;  \* Реализован fallback:



&nbsp;    \* при деградации качества модели или недоступности ML-сервиса система откатывается на классический HPA по текущим метрикам.



Архитектурная схема (high-level диаграмма) расположена в `docs/architecture-diagram.png`.



---



\## 4. Сравнение подходов к автомасштабированию



В проекте используются и сравниваются следующие подходы (подробная таблица в `docs/model-comparison-table.png`):



\* Реактивный HPA — baseline, низкая сложность, минимальные вычислительные затраты, но нет прогноза.

\* Прогнозирование с LSTM/GRU — проактивное масштабирование, средне-высокая точность, средние вычислительные затраты.

\* Прогнозирование с Prophet — отлично работает для сезонных данных, низкие вычислительные затраты.

\* Гибрид Prophet+LSTM — наивысшая точность (улучшение ≈6–15% по RMSE).

\* Reinforcement Learning — потенциально максимальная гибкость, но очень высокая сложность и вычислительная стоимость (опциональный этап).



Также подготовлена таблица рекомендаций и приоритета задач (см. `docs/recommendations-table.png`), включающая архитектуру ML-модели, сбор данных, интеграцию с Kubernetes, мониторинг, отказоустойчивость, безопасность и производительность.



---



\## 5. Стек технологий



Планируемый стек (может уточняться по ходу работы):



\* \*\*Язык / ML\*\*



&nbsp; \* Python 3.11+

&nbsp; \* PyTorch или TensorFlow/Keras (для LSTM/GRU)

&nbsp; \* Facebook Prophet / NeuralProphet

&nbsp; \* ONNX / ONNX Runtime (оптимизированный inference)



\* \*\*Инфраструктура и оркестрация\*\*



&nbsp; \* Kubernetes (kind / minikube для локальных экспериментов)

&nbsp; \* Kubernetes HPA / KEDA

&nbsp; \* Prometheus (сбор метрик)

&nbsp; \* Grafana (дашборды)

&nbsp; \* (опционально) k6 / Locust для нагрузочного тестирования



\* \*\*Сервисный слой\*\*



&nbsp; \* FastAPI (REST API для инференса и планировщика)

&nbsp; \* Python Kubernetes client (работа с K8s API)

&nbsp; \* Docker / Docker Compose для локального поднятия окружения



---



\## 6. Структура репозитория



Планируемая структура (может слегка меняться по мере развития):



models/

lstm/         — обучение и чекпоинты LSTM/GRU

prophet/      — конфигурации и скрипты Prophet

hybrid/       — реализация гибридной модели Prophet+LSTM

onnx/         — экспортированные модели для быстрого инференса



data/

raw/          — сырые выгрузки из Prometheus

processed/    — очищенные и подготовленные временные ряды



src/

collector/    — клиент Prometheus, загрузка и сохранение данных

preprocessor/ — препроцессинг: агрегация, аномалии, sliding window, декомпозиция

predictor/    — сервис инференса (FastAPI + ONNX Runtime)

planner/      — логика перевода прогноза в ресурсы (реплики, лимиты)

executor/     — интеграция с Kubernetes API / HPA / KEDA



k8s/

manifests/    — демо-микросервисы, Prometheus, Grafana и т.д.

hpa/          — конфигурация HPA c external/custom metrics

keda/         — `ScaledObject` и триггеры KEDA



dashboard/

grafana.json          — основной Grafana-дашборд

prometheus-rules.yml  — алерты и правила



notebooks/

research-data.ipynb       — EDA и работа с временными рядами

research-lstm.ipynb       — эксперименты с LSTM/GRU

research-prophet.ipynb    — эксперименты с Prophet

comparison.ipynb          — сравнение моделей и визуализация



docs/

overview-ru.md            — краткое описание проекта по-русски для научрука/кафедры

architecture-diagram.png  — диаграмма архитектуры

model-comparison-table.png

recommendations-table.png

experiments.md            — описание экспериментальной методики и результатов

literature-review.md      — обзор литературы и существующих подходов

paper-draft.md            — черновик научной статьи / НИР



Дополнительные файлы:



\* README.md (текущий файл)

\* ROADMAP.md (декларация фаз проекта и задач)

\* LICENSE (лицензия, по умолчанию можно взять MIT)

\* .gitignore



---



\## 7. Быстрый старт (план)



Конкретные команды будут добавлены по мере реализации. Примерный сценарий:



1\. Клонировать репозиторий:



&nbsp;  git clone \[https://github.com/USERNAME/k8s-ml-predictive-autoscaling.git](https://github.com/USERNAME/k8s-ml-predictive-autoscaling.git)

&nbsp;  cd k8s-ml-predictive-autoscaling



2\. Создать и активировать виртуальное окружение / установить зависимости:



&nbsp;  # TODO: будет добавлен requirements.txt или pyproject.toml



&nbsp;  pip install -r requirements.txt



3\. Поднять демо-окружение:



&nbsp;  # TODO: docker-compose для Prometheus, Grafana и демо-сервисов



&nbsp;  docker compose up -d



4\. Собрать данные из Prometheus:



&nbsp;  python -m src.collector.run



5\. Запустить препроцессинг и обучение базовых моделей (Prophet / LSTM):



&nbsp;  python -m src.preprocessor.run

&nbsp;  python -m models.prophet.train

&nbsp;  python -m models.lstm.train



6\. Запустить сервис инференса и планировщик:



&nbsp;  uvicorn src.predictor.app:app --reload

&nbsp;  uvicorn src.planner.app:app --reload



7\. Включить интеграцию с Kubernetes:



&nbsp;  # применение манифестов будет описано в k8s/manifests/



&nbsp;  kubectl apply -f k8s/manifests/



Подробный, пошаговый гайд будет добавлен после реализации Phase 0–2 (см. ROADMAP.md).



---



\## 8. Roadmap



Подробный план развития проекта вынесен в отдельный файл `ROADMAP.md`. Кратко по фазам:



\* Phase 0 — каркас репозитория, базовые контейнеры (Prometheus, Grafana, demo-services).

\* Phase 1 — сбор и препроцессинг данных, EDA.

\* Phase 2 — базовые модели Prophet и LSTM/GRU, сравнение.

\* Phase 3 — гибрид Prophet+LSTM, экспорт в ONNX.

\* Phase 4 — сервис инференса (FastAPI + ONNX Runtime).

\* Phase 5 — Resource Planner и интеграция с Kubernetes (HPA/KEDA).

\* Phase 6 — дашборды и мониторинг.

\* Phase 7 — нагрузочные эксперименты и оформление научных результатов.

\* Phase 8 — (опционально) RL-подход к автомасштабированию.



---



\## 9. Как использовать проект как научную работу



Этот репозиторий можно использовать как основу для:



\* НИР / курсовой / ВКР по направлениям:



&nbsp; \* распределённые системы, cloud computing;

&nbsp; \* машинное обучение в системах управления;

&nbsp; \* DevOps / SRE / autoscaling;

\* Научной статьи (РИНЦ, конференции, журналы):



&nbsp; \* результаты экспериментов, графики, таблицы сравнения;

&nbsp; \* описание архитектуры и методики.



В `docs/overview-ru.md` сформулированы актуальность, цель и задачи на академическом языке; файл можно показать потенциальному научному руководителю или кафедре.



---



\## 10. Статус



Статус: \*\*ранний research-прототип (MVP в разработке)\*\*.



На первых этапах будут реализованы:



\* каркас репозитория и окружения;

\* сбор данных из Prometheus;

\* базовые модели Prophet и LSTM;

\* начальные эксперименты с прогнозом нагрузки.



Дальнейшие шаги и прогресс см. в `ROADMAP.md` и в разделе Issues/Projects репозитория (будет заведено по мере разработки).



---



\## 11. Обратная связь и вклад



Пока проект делается как учебно-научный, но структура репозитория изначально ориентирована на открытый исходный код и возможность внешних вкладов.



Планы:



\* оформить CONTRIBUTING.md (код-стайл, требования к PR, структура экспериментов);

\* добавить шаблоны Issue и Pull Request;

\* описать, как повторить эксперименты и сравнения из статьи.



Если вы увидели этот репозиторий и вам интересна тема предиктивного автомасштабирования — можете открывать Issue с идеями, замечаниями и предложениями по улучшению.



