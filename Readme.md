# K8s ML Predictive Autoscaling

Исследовательский и инженерный проект по **ML-основанному предиктивному автомасштабированию** микросервисов в Kubernetes.

Цель — перейти от классического **реактивного масштабирования (HPA по CPU/памяти)** к **проактивному планированию ресурсов на основе прогноза нагрузки**, чтобы:

* удерживать латентность и SLO под контролем;
* снижать инфраструктурные затраты;
* получить воспроизводимую экспериментальную установку для научной работы (НИР, курсовая, ВКР, публикация).

> Кратко: здесь исследуется, насколько модели Prophet / LSTM / гибрид Prophet+LSTM могут улучшить работу Kubernetes-автомасштабирования по сравнению с обычным HPA.

---

## 1. Мотивация и постановка задачи

Современные системы автомасштабирования (например, Kubernetes Horizontal Pod Autoscaler) в основном используют **реактивный подход**: решение о масштабировании принимается по текущим метрикам CPU/памяти, часто с задержкой 30–60 секунд. Это приводит к:

* временной деградации производительности при всплесках нагрузки;
* росту латентности и нарушению SLA/SLO;
* эффекту «осцилляций» (частое масштабирование вверх/вниз);
* неоптимальному использованию ресурсов и лишним затратам.

**Предиктивное масштабирование** использует методы машинного обучения для прогноза нагрузки на горизонте нескольких минут и проактивного изменения количества реплик и лимитов ресурсов до наступления пиков.

В рамках проекта рассматриваются и сравниваются подходы:

* Реактивный HPA (baseline).
* Прогнозирование временных рядов: LSTM / GRU.
* Prophet (модель с учётом тренда и сезонности).
* Гибрид Prophet + LSTM.
* (Опционально) RL-подходы к автомасштабированию.

---

## 2. Цели и задачи проекта

### Цель

Разработать и исследовать систему предиктивного автомасштабирования микросервисов в Kubernetes на основе ML-прогноза нагрузки и сравнить её с классическим HPA по метрикам качества, производительности и стоимости.

### Основные задачи

1. Собрать и подготовить временные ряды нагрузки из Prometheus (CPU, память, RPS, latency, ошибки и т.д.).
2. Обучить и сравнить модели Prophet, LSTM/GRU и гибрид Prophet+LSTM по точности прогноза.
3. Реализовать сервис онлайн-прогноза нагрузки (FastAPI + ONNX Runtime).
4. Разработать модуль планирования ресурсов (Resource Planner), который переводит прогноз в:
   * количество реплик;
   * CPU/memory requests & limits;
   * политики ramp-up / ramp-down.
5. Интегрировать ML-прогнозы с Kubernetes через:
   * HPA external/custom metrics;
   * KEDA (event-driven autoscaling).
6. Построить Grafana-дашборд для визуализации:
   * реальной нагрузки vs. прогноз;
   * решений по масштабированию;
   * экономии ресурсов и SLO-нарушений.
7. Провести серию экспериментов с нагрузочным тестированием и формализовать результаты для научной публикации.

---

## 3. Общая архитектура

Система следует паттерну **MAPE loop**: **Monitor → Analyze → Plan → Execute**.

### 1. Monitor

* Микросервисы экспонируют `/metrics`.
* Prometheus собирает:
  * CPU, память;
  * network I/O;
  * request rate;
  * latency (p50, p95, p99);
  * error rate, custom-метрики приложения.
* Данные сохраняются как временные ряды и экспортируются для обучения/инференса.

### 2. Analyze (ML-слой)

* Препроцессинг данных:
  * агрегация по фиксированному шагу (10–60 секунд);
  * фильтрация аномалий (DDoS, synthetic load, ошибки мониторинга);
  * sliding window генерация;
  * опционально — декомпозиция временных рядов (например, CEEMDAN).
* Модели прогнозирования:
  * LSTM / GRU для нелинейных паттернов;
  * Prophet для трендов и сезонности;
  * гибрид Prophet+LSTM (Prophet моделирует тренд/сезонность, LSTM — остатки).
* Выход: прогноз нагрузки на горизонте 5–30 минут + доверительные интервалы.

### 3. Plan (Resource Planner)

* Конвертирует прогноз в план ресурсов:
  * целевое количество реплик;
  * лимиты CPU/памяти;
  * стратегия ramp-up / ramp-down;
  * небольшой over-provisioning (например, 10–15%) для надёжности.
* Учитывает бизнес-ограничения:
  * минимальное/максимальное число реплик;
  * SLO по латентности;
  * бюджет ресурсов.

### 4. Execute (интеграция с Kubernetes)

* ML-прогнозы экспортируются как **external/custom metrics** для HPA либо как источник для KEDA.
* Через Kubernetes API обновляются:
  * объекты `HorizontalPodAutoscaler`;
  * `ScaledObject` (для KEDA) или кастомные CRD.
* Реализован fallback:
  * при деградации качества модели или недоступности ML-сервиса система откатывается на классический HPA по текущим метрикам.

Архитектурная схема (high-level диаграмма) расположена в `docs/architecture-diagram.png`.

---

## 4. Сравнение подходов к автомасштабированию

В проекте используются и сравниваются следующие подходы (подробная таблица в `docs/model-comparison-table.png`):

* **Реактивный HPA** — baseline, низкая сложность, минимальные вычислительные затраты, но нет прогноза.
* **Прогнозирование с LSTM/GRU** — проактивное масштабирование, средне-высокая точность, средние вычислительные затраты.
* **Прогнозирование с Prophet** — отлично работает для сезонных данных, низкие вычислительные затраты.
* **Гибрид Prophet+LSTM** — наивысшая точность (улучшение ≈6–15% по RMSE).
* **Reinforcement Learning** — потенциально максимальная гибкость, но очень высокая сложность и вычислительная стоимость (опциональный этап).

---

## 5. Стек технологий

Планируемый стек (может уточняться по ходу работы):

### Язык / ML

* Python 3.11+
* PyTorch или TensorFlow/Keras (для LSTM/GRU)
* Facebook Prophet / NeuralProphet
* ONNX / ONNX Runtime (оптимизированный inference)

### Инфраструктура и оркестрация

* Kubernetes (kind / minikube для локальных экспериментов)
* Kubernetes HPA / KEDA
* Prometheus (сбор метрик)
* Grafana (дашборды)
* (опционально) k6 / Locust для нагрузочного тестирования

### Сервисный слой

* FastAPI (REST API для инференса и планировщика)
* Python Kubernetes client (работа с K8s API)
* Docker / Docker Compose для локального поднятия окружения

---

## 6. Структура репозитория

Планируемая структура (может слегка меняться по мере развития):

```
models/
  lstm/         — обучение и чекпоинты LSTM/GRU
  prophet/      — конфигурации и скрипты Prophet
  hybrid/       — реализация гибридной модели Prophet+LSTM
  onnx/         — экспортированные модели для быстрого инференса

data/
  raw/          — сырые выгрузки из Prometheus
  processed/    — очищенные и подготовленные временные ряды

src/
  collector/    — клиент Prometheus, загрузка и сохранение данных
  preprocessor/ — препроцессинг: агрегация, аномалии, sliding window, декомпозиция
  predictor/    — сервис инференса (FastAPI + ONNX Runtime)
  planner/      — логика перевода прогноза в ресурсы (реплики, лимиты)
  executor/     — интеграция с Kubernetes API / HPA / KEDA

k8s/
  manifests/    — демо-микросервисы, Prometheus, Grafana и т.д.
  hpa/          — конфигурация HPA c external/custom metrics
  keda/         — ScaledObject и триггеры KEDA

dashboard/
  grafana.json          — основной Grafana-дашборд
  prometheus-rules.yml  — алерты и правила

notebooks/
  research-data.ipynb       — EDA и работа с временными рядами
  research-lstm.ipynb       — эксперименты с LSTM/GRU
  research-prophet.ipynb    — эксперименты с Prophet
  comparison.ipynb          — сравнение моделей и визуализация

docs/
  overview-ru.md            — краткое описание проекта по-русски для научрука/кафедры
  architecture-diagram.png  — диаграмма архитектуры
  model-comparison-table.png
  recommendations-table.png
  experiments.md            — описание экспериментальной методики и результатов
  literature-review.md      — обзор литературы и существующих подходов
  paper-draft.md            — черновик научной статьи / НИР
```

Дополнительные файлы:

* README.md (текущий файл)
* ROADMAP.md (декларация фаз проекта и задач)
* LICENSE 
* .gitignore

---

## 7. Быстрый старт (план)

Конкретные команды будут добавлены по мере реализации. Примерный сценарий:

1. Клонировать репозиторий:
   ```bash
   git clone https://github.com/USERNAME/k8s-ml-predictive-autoscaling.git
   cd k8s-ml-predictive-autoscaling
   ```

2. Создать и активировать виртуальное окружение / установить зависимости:
   ```bash
   # TODO: будет добавлен requirements.txt или pyproject.toml
   pip install -r requirements.txt
   ```

3. Поднять демо-окружение:
   ```bash
   # TODO: docker-compose для Prometheus, Grafana и демо-сервисов
   docker compose up -d
   ```

4. Собрать данные из Prometheus:
   ```bash
   python -m src.collector.run
   ```

5. Запустить препроцессинг и обучение базовых моделей (Prophet / LSTM):
   ```bash
   python -m src.preprocessor.run
   python -m models.prophet.train
   python -m models.lstm.train
   ```

6. Запустить сервис инференса и планировщик:
   ```bash
   uvicorn src.predictor.app:app --reload
   uvicorn src.planner.app:app --reload
   ```

7. Включить интеграцию с Kubernetes:
   ```bash
   # применение манифестов будет описано в k8s/manifests/
   kubectl apply -f k8s/manifests/
   ```

Подробный, пошаговый гайд будет добавлен после реализации Phase 0–2 (см. ROADMAP.md).

---

## 8. Roadmap

Подробный план развития проекта вынесен в отдельный файл `ROADMAP.md`. Кратко по фазам:

* **Phase 0** — каркас репозитория, базовые контейнеры (Prometheus, Grafana, demo-services).
* **Phase 1** — сбор и препроцессинг данных, EDA.
* **Phase 2** — базовые модели Prophet и LSTM/GRU, сравнение.
* **Phase 3** — гибрид Prophet+LSTM, экспорт в ONNX.
* **Phase 4** — сервис инференса (FastAPI + ONNX Runtime).
* **Phase 5** — Resource Planner и интеграция с Kubernetes (HPA/KEDA).
* **Phase 6** — дашборды и мониторинг.
* **Phase 7** — нагрузочные эксперименты и оформление научных результатов.
* **Phase 8** — (опционально) RL-подход к автомасштабированию.

---

## 10. Статус

Статус: **ранний research-прототип (MVP в разработке)**.

На первых этапах будут реализованы:

* каркас репозитория и окружения;
* сбор данных из Prometheus;
* базовые модели Prophet и LSTM;
* начальные эксперименты с прогнозом нагрузки.

Дальнейшие шаги и прогресс см. в `ROADMAP.md` и в разделе Issues/Projects репозитория (будет заведено по мере разработки).

---

## 11. Обратная связь и вклад

Пока проект делается как учебно-научный, но структура репозитория изначально ориентирована на открытый исходный код и возможность внешних вкладов.

Планы:

* оформить CONTRIBUTING.md (код-стайл, требования к PR, структура экспериментов);
* добавить шаблоны Issue и Pull Request;
* описать, как повторить эксперименты и сравнения из статьи.

Если вы увидели этот репозиторий и вам интересна тема предиктивного автомасштабирования — можете открывать Issue с идеями, замечаниями и предложениями по улучшению.

---

## Лицензия

MIT License (детали в файле LICENSE)
